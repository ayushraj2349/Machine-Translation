{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5qMPrnjFSnx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "if (device == 'cuda'):\n",
        "  print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "  print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "device = torch.device(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee5VsSbgidD",
        "outputId": "cf22d6cc-feb6-4c5b-ec44-782b0c2a53e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: NVIDIA GeForce RTX 4080\n",
            "Device memory: 15.99169921875 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    PE(pos,2i) = sin(pos/10000^(2i/d_model))\n",
        "    PE(pos,2i+1) = cos(pos/10000^(2i+1/d_model))\n",
        "\"\"\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, seq_len, dropout):\n",
        "    super().__init__()          #__init__() call to the parent class must be made before assignment on the child inherited from nn.Module Class\n",
        "    self.d_model= d_model\n",
        "    self.seq_len= seq_len\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "\n",
        "    # Create a matrix of shape (seq_len, d_model)\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    # Create a vector of shape (seq_len)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)    # position\n",
        "    # Create a vector of shape (d_model)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))#10000^(-2i/d_model) == e^ ln(2i*ln(10000) / d_model)\n",
        "\n",
        "    # Apply sine to even indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "    # Apply cosine to odd indices\n",
        "    pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "    # Add a batch dimension to the positional encoding\n",
        "    pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "    # Register the positional encoding as a buffer\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # dim of input word embedding(x) -> (batch, seq_len, d_model)\n",
        "    return self.dropout(x)    # x -> (batch, seq_len, d_model) here seq_len is actual length of different sentences\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features, eps=10**-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(features)) # gamma is a learnable parameter\n",
        "        self.beta = nn.Parameter(torch.zeros(features)) # beta is a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "        # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        y = (x - mean) / (std + self.eps)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.gamma * y + self.beta"
      ],
      "metadata": {
        "id": "XKYRvdmvFhLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, h, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        # batch, h ,seq_len, d_k @ batch, h, d_k, seq_len == batch, h, seq_len, seq_len\n",
        "\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9) # PyTorch Function: Fills elements of attention_scores with value where mask==0\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax,import torch has inbuilt softmax\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores  # @ denotes matmul, attention_scores is returned here for visualization\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (bat ch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "        # for transforming a tensor, it needs to be contiguous in memory\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "HCDaIUhculbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1, nn.linear(in_features, out_features)\n",
        "        self.dropout  = nn.Dropout(dropout) # d_ff is 2048, acc to paper\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2, bias is true by default in nn.Linear\n",
        "        self.gelu     = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(self.gelu(self.linear_1(x))))\n",
        "\n",
        "## NOTE: Changed ReLU by GELU for activation function\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features, dropout):\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "97qecokl_0R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "pxRVhk7kLk38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask): # src_mask is for encoder and tgt_mask is for decoder\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)) # why src_mask here?\n",
        "        # key and value come from encoder and query from decoder\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features, layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask) # forward method of DecoderBlock\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "qfYOPnofQGq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module): # projecting the embedding into the vocabulary\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer #ProjectionLayer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src) # PositionalEncoding adds with word embeddings in the function itself\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask): # all the arguments are torch.tensor\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask) # forward method of DecoderBlock\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "_SkluQz1kIai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model=512, N=6, h=8, dropout=0.1, d_ff=2048)->Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "    # now, src_pos and tgt_pos are the resultant of word and positional embeddings\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout) #__init__\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block= DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)#__init__\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks)) #__init__\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks)) #__init__\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "RD8NlBbekQmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Importing contractions\n",
        "with open(\"Downloads/Machine Translation/contractions.txt\", \"r\") as inp_cont:\n",
        "    contractions = inp_cont.read()\n",
        "\n",
        "# print(contractions)\n",
        "#re.sub(r\"(.*{)|(}.*)\", '', contractions_list)--> removes everything before the first { and after the last }\n",
        "'''contractions = {\n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\"\n",
        "}\n",
        "'''\n",
        "\n",
        "''' after processing -->\n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\"\n",
        "'''\n",
        "#re.sub(r\"\\s+\", \" \"        --> replaces multiple whitespace characters with a single space.\n",
        "'''\n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\"\n",
        "'''\n",
        "# .split(',') --> This splits the string at each comma, creating a list of strings\n",
        "'''\n",
        "[\n",
        "    '\"ain't\": \"am not / are not / is not / has not / have not\"',\n",
        "    '\"aren't\": \"are not / am not\"'\n",
        "] '''\n",
        "\n",
        "# Each string in this list is further processed with re.sub('[\"]', '', x) to remove double quotes \" and\n",
        "# then split by colons split(\":\") to create a list of key-value pairs.\n",
        "'''\n",
        "[\n",
        "  ['ain't', ' am not / are not / is not / has not / have not']\n",
        "  ['aren't', ' are not / am not']\n",
        "]\n",
        "'''\n",
        "contractions_list = [re.sub('[\"]', '', x).split(\":\") for x in re.sub(r\"\\s+\", \" \", re.sub(r\"(.*{)|(}.*)\", '', contractions)).split(',')]\n",
        "\n",
        "'''\n",
        "contractions_list= [\n",
        "  ['ain't', ' am not / are not / is not / has not / have not']\n",
        "  ['aren't', ' are not / am not']\n",
        "]\n",
        "'''\n",
        "# k.lower().strip() ensures that keys (contractions) are in lowercase and stripped of leading/trailing whitespace.\n",
        "# for values(v), it removes everything after '/' in that and after processes it similarly lowercasing and removing whitespaces.\n",
        "'''\n",
        "contractions_dict= {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\"\n",
        "}\n",
        "'''\n",
        "contractions_dict = dict((k.lower().strip(), re.sub('/.*', '', v).lower().strip()) for k, v in contractions_list)\n",
        "\n",
        "#print(contractions_list)\n",
        "#print(contractions_dict)\n",
        "\n",
        "\n",
        "def remove_sc(_line, lang=\"en\"):\n",
        "\n",
        "    if lang == \"hi\":\n",
        "        _line = re.sub(r'[+\\-*/#@%>=;~{}×–`’\"()_]', \"\", _line)\n",
        "        _line = re.sub(r\"(?:(\\[)|(\\])|(‘‘)|(’’))\", '', _line)\n",
        "        # ](\\[) OR [(\\]) OR \"(‘‘) OR \"(’’)\n",
        "    elif lang == \"en\":\n",
        "        _line = re.sub(r'[+\\-*/#@%>=;~{}×–`’\"()_|:]', \"\", _line)\n",
        "        _line = re.sub(r\"(?:(\\[)|(\\])|(‘‘)|(’’))\", '', _line)\n",
        "    return _line\n",
        "\n",
        "def clean_text(_text, lang=\"en\"):\n",
        "\n",
        "    if lang == \"en\":\n",
        "        _text = remove_sc(_line=_text, lang=lang)\n",
        "        _text= re.sub(r\"\\s'(\\w)\", r\"'\\1\", _text) # removing whitespaces between the contractions\n",
        "        for cn in contractions_dict:\n",
        "            _text = re.sub(cn, contractions_dict[cn], _text)\n",
        "    elif lang == \"hi\":\n",
        "        _text = remove_sc(_line=_text, lang=lang)\n",
        "    return _text"
      ],
      "metadata": {
        "id": "tkV18Q5e4RRF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.tokenizer_src.encode(clean_text(src_text, self.src_lang)).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(clean_text(tgt_text, self.tgt_lang)).ids\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            \"label\": label,  # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ],
      "metadata": {
        "id": "_ulybcNP2co2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 16,\n",
        "        \"num_epochs\": 16,\n",
        "        \"lr\": 1e-4,\n",
        "        \"seq_len\": 310,\n",
        "        \"d_model\": 512,\n",
        "        \"datasource\": 'cfilt/iitb-english-hindi',\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"hi\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "    return str(Path('Downloads/Machine Translation') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"# 'model_' --> 'model_*'\n",
        "    # * is a wildcard character that matches any sequence of characters in a filename.\n",
        "    weights_files = list(Path(f\"Downloads/Machine Translation/{model_folder}\").glob(model_filename)) #it will search for files with name 'model_'\n",
        "    # print(weights_files)\n",
        "\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1]) #return the path to the latest file, which will be the last one"
      ],
      "metadata": {
        "id": "ETiaw_3h_n9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets tokenizers"
      ],
      "metadata": {
        "id": "-2uM2HjIASoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca50058-f63b-42bb-ec32-7808240695c5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: tokenizers in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.24.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pushkar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# HuggingFace datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, normalizers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, num_examples=3):\n",
        "    model.eval() # evaluation mode\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask =  batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        pre_processed_text = clean_text(item['translation'][lang], lang)\n",
        "        yield pre_processed_text\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    directory_path = f'Downloads/Machine Translation/{config[\"tokenizer_file\"].format(lang)}'\n",
        "    tokenizer_path = Path(directory_path)\n",
        "\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "\n",
        "        if lang == 'en':\n",
        "          tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase(),\n",
        "                                                        normalizers.NFD(),\n",
        "                                                        normalizers.StripAccents()])\n",
        "\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "def get_ds(config):\n",
        "    # we divide the train split into train and val splits\n",
        "    ds_raw = load_dataset(f\"{config['datasource']}\") # please check the split parameter here\n",
        "\n",
        "    train_ds_raw = ds_raw['train'].select(range(80000)) # in this way, we can select the no. of examples as much as desired\n",
        "    val_ds_raw   = ds_raw['validation']\n",
        "    test_ds_raw  = ds_raw['test']\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, train_ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, train_ds_raw, config['lang_tgt'])\n",
        "\n",
        "\n",
        "    '''\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "    '''\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in train_ds_raw:\n",
        "        src_ids = tokenizer_src.encode(clean_text(item['translation'][config['lang_src']], config['lang_src'])).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(clean_text(item['translation'][config['lang_tgt']], config['lang_tgt'])).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader   = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()"
      ],
      "metadata": {
        "id": "gkTU-hm82zZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lrate= d_model ^(-0.5) * min(step_num^(-0.5), step_num* warmup_step *(-1.5))\n",
        "# This corresponds to increasing the learning rate linearly for the first warmup_steps training steps\n",
        "# and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps=3.\n",
        "\n",
        "def rate(step, d_model, factor, warmup):\n",
        "    \"\"\"\n",
        "    we have to default the step to 1 for LambdaLR function\n",
        "    to avoid zero raising to negative power.\n",
        "    \"\"\"\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    return factor * (\n",
        "        d_model ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
        "    )"
      ],
      "metadata": {
        "id": "zuWzdHZckYoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
        "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    weight_path = f\"Downloads/Machine Translation/{config['model_folder']}\"\n",
        "    Path(weight_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-8)\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    # from torch.optim.lr_scheduler import LambdaLR\n",
        "    # scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: rate(step, *example))\n",
        "    # but we won't be using the proposed scheduler in paper here because we're not training for very large no. of epochs\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 0\n",
        "    preload = config['preload']\n",
        "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
        "\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        #scheduler.load_state_dict(state['scheduler_state_dict'])\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device) #used LabelSmoothing\n",
        "\n",
        "    train_loss = []\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        losses = []\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train() # training mode\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask  = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask  = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output    = model.project(decoder_output) # (B, seq_len, tgt_vocab_size) #this is the o/p of the model\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            # proj_o/p --> (B, seq_len, tgt_vocab_size) --> (B*seq_len, tgt_vocab_size)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)) # computing the loss\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Clipping to avoid exploding gradient issues, makes sure grads are\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        train_mean_loss = sum(losses) / len(losses)\n",
        "        train_loss.append(train_mean_loss)\n",
        "        #scheduler.step(train_mean_loss)\n",
        "        print(f\"Epoch #{epoch} Training Loss: {train_mean_loss}\")\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg))\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #'scheduler_state_dict': scheduler.state_dict(),\n",
        "        }, model_filename)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ],
      "metadata": {
        "id": "riBLIfuykXGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e28f3e9-fe25-486f-c0b7-946b622adfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: NVIDIA GeForce RTX 4080\n",
            "Device memory: 15.99169921875 GB\n",
            "Max length of source sentence: 261\n",
            "Max length of target sentence: 157\n",
            "No model to preload, starting from scratch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|█████████████████████████████████████████████| 5000/5000 [14:05<00:00,  5.91it/s, loss=3.088]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0 Training Loss: 4.088024450564385\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The decision regarding the rest of the candidates for the Assembly would only be made after Diwali.\n",
            "    TARGET: बाकी सीटों पर फैसला दीपावली बाद जिले की बाकी विधानसभा सीटों पर प्रत्याशियों की घोषणा दीपावली के बाद होने की संभावना व्यक्त की जा रही है।\n",
            " PREDICTED: के द्वारा स्थिति में है के लिए के लिए है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: It is clear that the DTO office itself accepts that DLs are being delivered after one month, while the service act specifies that the deadline is seven days.\n",
            "    TARGET: साफ है कि डीटीओ ऑफिस खुद मान रहा है कि डीएल एक महीने के बाद डिलीवर हो रहे हैं, जबकि राइट टू सर्विस एक्ट में इसकी डेडलाइन सात दिन तय की गई है।\n",
            " PREDICTED: आपका सेवा साइट में तक पहले से पहले पहले से पहले है . कि फिर से भी कोशिश करें\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The people of the community rushed to the spot.\n",
            "    TARGET: मुहल्ले वाले भी भागे।\n",
            " PREDICTED: के लिए हैं .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 01: 100%|█████████████████████████████████████████████| 5000/5000 [14:11<00:00,  5.87it/s, loss=1.550]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1 Training Loss: 2.192030510568619\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Harish left his tenth grade studies midway.\n",
            "    TARGET: हरीश 10वीं की पढ़ाई बीच में छोड़ रखी है।\n",
            " PREDICTED: \n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: This change has been made after taking into consideration the international fluctuation of prices in both the precious metals.\n",
            "    TARGET: दोनों कीमती धातुओं के वैश्रि्वक मूल्य में उतार-चढ़ाव के मद्देनजर इसमें बदलाव किया गया है।\n",
            " PREDICTED: यह परिवर्तन के बाद में दिखाने के लिए प्रयुक्त था .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: On hearing the sound of a car colliding with a tree, the near-by residents came to the scene and immediately informed the police.\n",
            "    TARGET: गाड़ी के पेड़ से टकराने की आवाज सुनकर आसपास के लोग मौके पर पहुंच गए तथा उन्होंने पुलिस को सूचित किया।\n",
            " PREDICTED: के पहले एक ध्वनि का a को a को के साथ एक बार\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 02: 100%|█████████████████████████████████████████████| 5000/5000 [14:08<00:00,  5.90it/s, loss=1.442]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #2 Training Loss: 1.7273695035219192\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: By making these beautiful rangolis you can also make your house beautiful.\n",
            "    TARGET: ये खूबसूरत रंगोलियां बनाकर आप भी अपने घर की खूबसूरती बढ़ा सकते हैं।\n",
            " PREDICTED: \n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: During this time they discussed the combined activities of the party supporters and the members.\n",
            "    TARGET: उन्होंने इस दौरान पार्टी समर्थकों व सदस्यों से सांगठनिक गतिविधि पर चर्चा की।\n",
            " PREDICTED: इस समय के दौरान दिखाई देने के दौरान दिखाई देता है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Only preaching or messages from Rahul-Sonia seem to work for Congress.\n",
            "    TARGET: राहुल-सोनिया के संदेश या उपदेश ही कांग्रेस के काम आते हैं।\n",
            " PREDICTED: केवल संदेश के लिए में\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 03: 100%|█████████████████████████████████████████████| 5000/5000 [13:52<00:00,  6.00it/s, loss=1.560]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #3 Training Loss: 1.5789985405921936\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The speeding jeep collided with the back of truck.\n",
            "    TARGET: तेज रफ्तार जीप ट्रक के पिछले हिस्से में जा घुसी।\n",
            " PREDICTED: के साथ की\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: At the same time he said, and shocked everyone by saying, that until he is called to the excavation site the gold will not be found.\n",
            "    TARGET: वहीं पर कभी यह कहकर सबको चौंका दिया कि जब तक उन्हें खुदाई स्थल पर नहीं बुलाया जाता तब तक सोना नहीं निकलने वाला है।\n",
            " PREDICTED: एक समय में अब भी हो , , , जायेगा , वह एक असुरक्षित है , वह उपयोग में नहीं भेजा जायेगा , तो आपके पास नहीं है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The shares of consumer goods, banking, metal, oil, natural gas and power has helped the market to pick up.\n",
            "    TARGET: बाजार में इस तेजी की अगुवाई कंज्यूमर गुड्स,बैकिंग,धातु, तेल एंव गैस तथा पावर वर्ग के शेयरों ने की ।\n",
            " PREDICTED: की , , , , में में\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 04: 100%|█████████████████████████████████████████████| 5000/5000 [13:51<00:00,  6.02it/s, loss=1.417]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #4 Training Loss: 1.5054933074235917\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Jagran correspondent, Agra: The Fatehabad road had a deadly day on Thursday when there were several fatal accidents.\n",
            "    TARGET: जागरण संवाददाता, आगराः फतेहाबाद रोड पर गुरुवार रात हादसों की शक्ल में मौत मंडराती रही।\n",
            " PREDICTED: \n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Utensil seller, Rajesh said that this market has the maximum number of utensil shops.\n",
            "    TARGET: बर्तन विक्रेता राजेश का कहना था कि इस बजार में सबसे ज्यादा बर्तन की दुकाने हैं।\n",
            " PREDICTED: , इस संख्या को पहले जैसा किए जाने के लिए प्रयुक्त है\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: This Delegation also met with MPP Wick Dhilon earlier and soon they will also meet with other MPs and MPPs.\n",
            "    TARGET: इससे पहले डेलिगेशन एमपीपी विक ढिल्लों को भी मिल चुका है और आने वाले दिनों में कई अन्य एमपी और एमपीपी से भी मिलेगा।\n",
            " PREDICTED: यह\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 05: 100%|█████████████████████████████████████████████| 5000/5000 [13:51<00:00,  6.01it/s, loss=1.437]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #5 Training Loss: 1.46092484767437\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: He has been admitted to Sundernagar civil hospital.\n",
            "    TARGET: चालक को नागरिक अस्पताल सुंदरनगर में भर्ती करवाया गया है।\n",
            " PREDICTED: का तरीका को में है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: DIET Principal Pradeep Sharma conducted a surprise inspection of schools in the Thanamandi zone to check the midday meals there.\n",
            "    TARGET: डाइट प्रिंसिपल प्रदीप शर्मा ने थन्ना मंडी जोन के सरकारी स्कूलों का औचक दौरा कर स्कूलों में मिडडे मील की जांच की।\n",
            " PREDICTED: \n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: In one program, while praising the beauty of the Sultanpur D.M., he said \"I am very fortunate that I have become the minister in charge of this zone for a second time.\n",
            "    TARGET: एक कार्यक्रम में सुल्तानपुर की डीएम की खूबसूरती की तारीफ करते हुए उन्होंने कहा था, 'यह मेरा सौभाग्य है कि मैं दूसरी बार इस जिले का प्रभारी मंत्री बना हूं।\n",
            " PREDICTED: एक प्रोग्राम में से एक का संस्करण को इस का संस्करण को फिर से अधिक है . को फिर से आरंभ किया जा रहा है जब तक कि का डाउनलोड करने में जब तक कि रहा है .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 06: 100%|█████████████████████████████████████████████| 5000/5000 [13:51<00:00,  6.01it/s, loss=1.497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #6 Training Loss: 1.4320751959562301\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Former UP Minister, Rajaram Pandey, who was known for his controversial speeches, passed away late Thursday evening following a heart attack.\n",
            "    TARGET: अपने विवादित बायनों के लिए चर्चित हुए यूपी के पूर्व मंत्री राजाराम पांडे का गुरुवार देर रात हार्ट अटैक पड़ने से निधन हो गया।\n",
            " PREDICTED: , , , , , , , की करने के लिए , ,\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: This change has been made after taking into consideration the international fluctuation of prices in both the precious metals.\n",
            "    TARGET: दोनों कीमती धातुओं के वैश्रि्वक मूल्य में उतार-चढ़ाव के मद्देनजर इसमें बदलाव किया गया है।\n",
            " PREDICTED: यह परिवर्तन को पश्चात है\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: According to the details received, Komal's father had passed away a few years ago, her mother is mentally ill and her brother is studying in a government school.\n",
            "    TARGET: विवरण के अनुसार कोमल के पिता की कई साल पहले मृत्यु हो चुकी है उसकी मां, मानसिक रोगी है और एक भाई सरकारी स्कूल में पढ़ता है।\n",
            " PREDICTED: इस प्रकार की स्थिति में IMAP सर्वर से समायोजित करें , लेकिन आपके पास कोई नहीं है . क्या आप इसे डाउनलोड कर सकते हैं .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 07: 100%|█████████████████████████████████████████████| 5000/5000 [13:51<00:00,  6.01it/s, loss=1.562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #7 Training Loss: 1.410319785284996\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The investigation was done before Diwali on the 10th of November at different times.\n",
            "    TARGET: दिवाली के पहले 10 नवंबर को दो अलग-अलग समय पर की गई\n",
            " PREDICTED: मेलबाक्स में से शुरू हो गया था .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Naming is a cognitive exercise.\n",
            "    TARGET: नामकरण ज्ञानबोधक होता है।\n",
            " PREDICTED: एक है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Also, in the B.D.O Office, apart from two temporary employees, the B.D.O himself along with his employee were not present.\n",
            "    TARGET: इसके साथ साथ बीडीओ कार्यालय में दो अस्थायी कर्मचारी को छोड़ कर बीडीओ सहित अन्य कर्मचारी कार्यालय में उपस्थित नहीं थे।\n",
            " PREDICTED: सेकेंड में , R संदेश प्राप्त किया जा रहा है , क्रैश से किसी दूसरे सिस्टम सर्वर से . नहीं है .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 08: 100%|█████████████████████████████████████████████| 5000/5000 [13:52<00:00,  6.00it/s, loss=1.347]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #8 Training Loss: 1.3937690151453017\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Because of this utensil sellers have already decorated their shops.\n",
            "    TARGET: जिसके चलते बाजार में बर्तन विक्रेताओं की दुकानें सज चुकी हैं।\n",
            " PREDICTED: इस पहले से ही मौजूद है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The NRI will have to give proof of ownership to the SDM and they will have to tell them their requirements.\n",
            "    TARGET: एनआरआई को जगह का मालिक होने का प्रमाण एसडीएम को देना होगा और अपनी जरूरत बतानी होगी।\n",
            " PREDICTED: पर को के की की के की को\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Parents were angry when the fees were increased once again.\n",
            "    TARGET: इस बार भी फिर फीस बढ़ाने से अभिभावकों में रोष पाया गया।\n",
            " PREDICTED: समय समाप्ति को फिर से कोशिश करें . www . com\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 09: 100%|█████████████████████████████████████████████| 5000/5000 [13:55<00:00,  5.98it/s, loss=1.493]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #9 Training Loss: 1.3803318328142167\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Councillor Anup Sav arrived.\n",
            "    TARGET: पार्षद अनूप साव पहुंचे।\n",
            " PREDICTED: का .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The people of India have made the country world renowned with their active efforts.\n",
            "    TARGET: भारत के लोगों ने ही अपने सचेत श्रम से राष्ट्र को विश्वप्रतिष्ठ किया है।\n",
            " PREDICTED: का दें के आपके पास कोई को निर्धारित करता है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The politicisation of the schemes was done by Congress itself.\n",
            "    TARGET: योजनाओं का राजनीतिकरण कांग्रेस ने ही किया।\n",
            " PREDICTED: छवि की स्लाइड शो की स्थिति के द्वारा लौटाया .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 10: 100%|█████████████████████████████████████████████| 5000/5000 [13:54<00:00,  5.99it/s, loss=1.369]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #10 Training Loss: 1.370163449382782\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: He said that people must buy Gold, Silver and other metal items on this day as their financial capacity allows.\n",
            "    TARGET: उन्होंने कहा कि इस दिन आर्थिक क्षमता के मुताबिक सोना, चांदी व अन्य धातु अवश्य खरीदना चाहिए।\n",
            " PREDICTED: , पर और के को , के की तरह\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The chief said that developmental work should be carried in keeping with transparency in the government work.\n",
            "    TARGET: प्रमुख ने कहा कि सरकारी कार्य में पारदर्शिता रखकर विकास कार्य को आगे बढ़ाने की जरूरत है।\n",
            " PREDICTED: , कार्य बनाने के लिए चुनते हैं , जो कि एक अंश पर मुद्रित करें .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: All those renewal renewed and valid driving licenses (DL) are ready where photos are donewere submitted by till up to the 30th of September, The rest will be provided by next week.\n",
            "    TARGET: रिन्युअल व पक्के ड्राइविंग लाइसेंस (डीएल) जिनकी फोटो ३० सितंबर तक हो चुकी है वह तैयार हो चुके हैं, बाकी लाइसेंस अगले सप्ताह मिलेंगे।\n",
            " PREDICTED: वे है और के लिए एक बार में एक ही मिटाना है , को पुन सेट करें , के सभी भी सटीक\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 11: 100%|█████████████████████████████████████████████| 5000/5000 [14:10<00:00,  5.88it/s, loss=1.355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #11 Training Loss: 1.3608799506425857\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: According to the description, the details of the death of Komal only came to light when one of Komal's cousin went to the third floor of their house to do some cleaning.\n",
            "    TARGET: विवरण के अनुसार कोमल द्वारा मौत को गले लगाने के घटनाक्रम का पता शुक्रवार देर दोपहर को उस समय लगा जब उसकी चचेरी बहन तीसरी मंजिल की छत पर बने कमरे में सफाई करने गई।\n",
            " PREDICTED: विवरण की समीक्षा , विवरण को से विवरण की से से का से अधिक से से , का , , , and है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Government schemes are run from the accumulated exchequer, from the country's taxpayers.\n",
            "    TARGET: सरकारी योजनाएं देश के करदाताओं से संचित राजकोष से चलती हैं।\n",
            " PREDICTED: से को , , ' s ' से ,\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Chauki in-charge Rejister Pal Singh, who was sitting on the front, was killed immediately.\n",
            "    TARGET: आगे बैठे चौकी प्रभारी रजिस्टर पाल सिंह की घटनास्थल पर ही मौत हो गई।\n",
            " PREDICTED: का , , , , , , शुरू हो जाएगा , तो ,\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 12: 100%|█████████████████████████████████████████████| 5000/5000 [13:52<00:00,  6.00it/s, loss=1.334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #12 Training Loss: 1.3549497336626053\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: If you are still not satisfied and you have reservations about these names, there are other options that are available in the market.\n",
            "    TARGET: यदि फिर भी बात न बने तो आपको इनके नामों से परहेज है तो चलिए आप के लिए बाजार में और भी विकल्प मौजूद हैं।\n",
            " PREDICTED: यदि आप नई परियोजना नहीं हैं तो आप रोस्टर में एकिगा निर्धारित हो तो वे अन्य विकल्प , उदाहरण के लिए चुन रहे हैं .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The bigger lie is the more effective.\n",
            "    TARGET: बड़ा झूठ ज्यादा प्रभावकारी होता है।\n",
            " PREDICTED: का अधिक है .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Urmila Malik, Geeta Gupta, Rajbir Singh and Amit Gupta were honoured as jury members.\n",
            "    TARGET: उर्मिला मलिक, गीता गुप्ता, राजबीर सिंह व अमित गुप्ता को निर्णायक के रूप में सम्मानित किया गया।\n",
            " PREDICTED: , , , , और , and\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 13:  75%|█████████████████████████████████▊           | 3754/5000 [10:07<03:21,  6.18it/s, loss=1.323]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m--> 105\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[19], line 70\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     66\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, seq_len)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Compute the loss using a simple cross entropy\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# proj_o/p --> (B, seq_len, tgt_vocab_size) --> (B*seq_len, tgt_vocab_size)\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_tgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# computing the loss\u001b[39;00m\n\u001b[0;32m     71\u001b[0m batch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     72\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    # Create a candidate list\n",
        "    candidates = [(decoder_initial_input, 1)]\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
        "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
        "            break\n",
        "\n",
        "        # Create a new list of candidates\n",
        "        new_candidates = []\n",
        "\n",
        "        for candidate, score in candidates:\n",
        "\n",
        "            # Do not expand candidates that have reached the eos token\n",
        "            if candidate[0][-1].item() == eos_idx:\n",
        "                continue\n",
        "\n",
        "            # Build the candidate's mask\n",
        "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
        "            # calculate output\n",
        "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
        "            # get next token probabilities\n",
        "            prob = model.project(out[:, -1])\n",
        "            # get the top k candidates\n",
        "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
        "            for i in range(beam_size):\n",
        "                # for each of the top k candidates, get the token and its probability\n",
        "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
        "                token_prob = topk_prob[0][i].item()\n",
        "                # create a new candidate by appending the token to the current candidate\n",
        "                new_candidate = torch.cat([candidate, token], dim=1)\n",
        "                # We sum the log probabilities because the probabilities are in log space\n",
        "                new_candidates.append((new_candidate, score + token_prob))\n",
        "\n",
        "        # Sort the new candidates by their score\n",
        "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "        # Keep only the top k candidates\n",
        "        candidates = candidates[:beam_size]\n",
        "\n",
        "        # If all the candidates have reached the eos token, stop\n",
        "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
        "            break\n",
        "\n",
        "    # Return the best candidate\n",
        "    return candidates[0][0].squeeze()"
      ],
      "metadata": {
        "id": "dh0rC0Be4cB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)"
      ],
      "metadata": {
        "id": "8KSkU3WwarjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, num_examples=3):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask  = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "\n",
        "            model_out_greedy = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "            model_out_beam = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n",
        "            model_out_text_greedy = tokenizer_tgt.decode(model_out_greedy.detach().cpu().numpy())\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>20}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>20}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED GREEDY: ':>20}{model_out_text_greedy}\")\n",
        "            print_msg(f\"{f'PREDICTED BEAM: ':>20}{model_out_text_beam}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "max_len = 20\n",
        "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, max_len, device, print_msg=print, num_examples=3)"
      ],
      "metadata": {
        "id": "JzCby1onO1ob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6bfc6c-627d-4be5-e736-d197bd6662f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "            SOURCE: Other than the precious metals the price of brass scrap, which was until yesterday $3,933 per ton, has been reduced to $3,840 per ton.\n",
            "            TARGET: कीमती धातुओं के अलावा पीतल स्क्रैप का शुल्क मूल्य घटाकर 3,840 डॉलर प्रति टन कर दिया गया जो कल तक 3,933 डॉलर प्रति टन था।\n",
            "  PREDICTED GREEDY: \n",
            "    PREDICTED BEAM: \n",
            "--------------------------------------------------------------------------------\n",
            "            SOURCE: This not only affects the children's education but also affects the parents' businesses.\n",
            "            TARGET: इससे बच्चों की पढ़ाई खराब होने के साथ-साथ उनके कारोबार पर भी प्रतिकूल असर पड़ता है।\n",
            "  PREDICTED GREEDY: यह टैग s एक से नहीं\n",
            "    PREDICTED BEAM: यह टैग s एक को से नहीं\n",
            "--------------------------------------------------------------------------------\n",
            "            SOURCE: People around the house immediately reported the matter to the police.\n",
            "            TARGET: तत्काल आसपास के लोगों ने इसकी सूचना पुलिस को दी।\n",
            "  PREDICTED GREEDY: की\n",
            "    PREDICTED BEAM: SSL असफल : s को में भेजने के लिए सूचना की कोशिश करता है . A\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_src.get_vocab_size())\n",
        "print(tokenizer_tgt.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOAv8Xa0_JXv",
        "outputId": "2f71ea38-043c-4743-c085-2262413fbd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3309\n",
            "3309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_config()"
      ],
      "metadata": {
        "id": "fhF3lfEmyOST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, normalizers\n",
        "\n",
        "src_directory_path = f\"/content/drive/MyDrive/Project Work/Machine Translation/{config['tokenizer_file'].format(config['lang_src'])}\"\n",
        "tgt_directory_path = f\"/content/drive/MyDrive/Project Work/Machine Translation/{config['tokenizer_file'].format(config['lang_tgt'])}\"\n",
        "\n",
        "tokenizer_src = Tokenizer.from_file(str(Path(src_directory_path)))\n",
        "tokenizer_tgt = Tokenizer.from_file(str(Path(tgt_directory_path)))"
      ],
      "metadata": {
        "id": "UDEuDbrDx_dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_src.token_to_id('[PAD]'))\n",
        "print(tokenizer_tgt.token_to_id('[PAD]'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPQxp50uyBH_",
        "outputId": "1419907d-5821-4501-a570-596d38f71674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "config = get_config()\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "\n",
        "print(tokenizer_src.get_vocab_size())\n",
        "print(tokenizer_src.get_vocab_size())\n",
        "\n",
        "model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
        "\n",
        "# Load the pretrained weights\n",
        "model_filename = get_weights_file_path(config, \"02\")\n",
        "checkpoint = torch.load(model_filename,  map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "epoch= checkpoint['epoch']\n",
        "\n",
        "model.to(device).eval()\n",
        "print(f'Successfully Loaded The Best Model, achieved on Epoch {epoch+1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbg6NDrz47hU",
        "outputId": "60bede16-ace3-4846-a60a-078b6279ffb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max length of source sentence: 261\n",
            "Max length of target sentence: 157\n",
            "4365\n",
            "4365\n",
            "Successfully Loaded The Best Model, achieved on Epoch 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "def translate(sentence: str):\n",
        "    # Define the device, tokenizers, and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    config = get_config()\n",
        "    src_directory_path = f\"Downloads/Machine Translation/{config['tokenizer_file'].format(config['lang_src'])}\"\n",
        "    tgt_directory_path = f\"Downloads/Machine Translation/{config['tokenizer_file'].format(config['lang_tgt'])}\"\n",
        "\n",
        "    tokenizer_src = Tokenizer.from_file(str(Path(src_directory_path)))\n",
        "    tokenizer_tgt = Tokenizer.from_file(str(Path(tgt_directory_path)))\n",
        "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
        "\n",
        "    # Load the pretrained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    checkpoint = torch.load(model_filename,  map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    epoch= checkpoint['epoch']\n",
        "    model.to(device).eval()\n",
        "    print(f'Successfully Loaded The Best Model, achieved on Epoch {epoch+1}')\n",
        "\n",
        "    # if the sentence is a number use it as an index to the test set\n",
        "    label = \"\"\n",
        "    if type(sentence) == int or sentence.isdigit():\n",
        "        id = int(sentence)\n",
        "        ds = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='all')\n",
        "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "        sentence = ds[id]['src_text']\n",
        "        label = ds[id][\"tgt_text\"]\n",
        "    seq_len = config['seq_len']\n",
        "\n",
        "    # translate the sentence\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Precompute the encoder output and reuse it for every generation step\n",
        "        source = tokenizer_src.encode(sentence)\n",
        "        source = torch.cat([\n",
        "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
        "            torch.tensor(source.ids, dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
        "        ], dim=0).to(device)\n",
        "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "        encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "        # Initialize the decoder input with the sos token\n",
        "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
        "\n",
        "        # Print the source sentence and target start prompt\n",
        "        if label != \"\": print(f\"{f'ID: ':>12}{id}\")\n",
        "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
        "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\")\n",
        "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
        "\n",
        "        # Generate the translation word by word\n",
        "        while decoder_input.size(1) < seq_len:\n",
        "            # build mask for target and calculate output\n",
        "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
        "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "            # project next token\n",
        "            prob = model.project(out[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "            # print the translated word\n",
        "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
        "\n",
        "            # break if we predict the end of sentence token\n",
        "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    # convert ids to tokens\n",
        "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
        "\n",
        "#read sentence from argument\n",
        "translate(\"how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "iEshPyMF1pED",
        "outputId": "9d74da8c-6390-407c-80af-e401d34d465f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Successfully Loaded The Best Model, achieved on Epoch 6\n",
            "    SOURCE: i love you\n",
            " PREDICTED:                                                                                                                                                                                                                                                                "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu"
      ],
      "metadata": {
        "id": "bPQe0BxpS9X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "def run_evaluation(test_ds, beam_size, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    # Define the tokenizers, and model\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    config = get_config()\n",
        "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
        "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
        "\n",
        "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(),\n",
        "                              config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
        "\n",
        "    # Load the pretrained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    state = torch.load(model_filename)\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    source_texts    = []\n",
        "    predictions     = []\n",
        "    target          = []\n",
        "\n",
        "    model.eval() # evaluation mode\n",
        "    with torch.no_grad():\n",
        "      for batch in test_ds:\n",
        "          encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "          encoder_mask  = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "          # check that the batch size is 1\n",
        "          assert encoder_input.size(\n",
        "              0) == 1, \"Batch size must be 1 for evaluation\"\n",
        "\n",
        "          model_out_beam = beam_search_decode(model, beam_size, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "          source_text = batch[\"src_text\"][0] #Get the first source text in the batch, it is the only one, as batch_size is 1\n",
        "          target_text = batch[\"tgt_text\"][0] #Get the first target text in the batch, it is the only one, as batch_size is 1\n",
        "          model_out_text = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n",
        "\n",
        "          source_texts.append(source_text)\n",
        "          target.append(target_text)\n",
        "          predictions.append(model_out_text)\n",
        "\n",
        "    target_list = [[ref] for ref in target]\n",
        "    sacrebleu = load_dataset('sacrebleu')\n",
        "    # SacreBLEU operates on raw text, not tokens\n",
        "    # that's why we've not used BLEU Metric here\n",
        "    score = sacrebleu.compute(predictions=predictions, references=target_list)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "n0IKO2hj5jJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_raw     = ds_raw['test']\n",
        "test_ds         = BilingualDataset(test_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "test_dataloader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "beam_size       = 3\n",
        "max_len         = 20\n",
        "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# seq_len or max_len is the max. no of words in a sentence\n",
        "# d_model is the dimension of vector embedding of each word\n",
        "\n",
        "bleu_score = run_evaluation(test_ds, beam_size, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "score      = round(bleu_score['score'],2)\n",
        "print(f\"BLEU Score on test_dataset: {score}\")\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "id": "OHAco8EWeiqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Visualization"
      ],
      "metadata": {
        "id": "0py7D4UeCzIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import altair as alt  # visualization library for charts\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "TJgOvHqaCLaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "15t7WF4yCNPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_config()\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the pretrained weights\n",
        "model_filename = get_weights_file_path(config, f\"29\")\n",
        "state = torch.load(model_filename)\n",
        "model.load_state_dict(state['model_state_dict'])"
      ],
      "metadata": {
        "id": "DgoPAYe2CPDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_next_batch():\n",
        "    # Load a sample batch from the validation set\n",
        "    batch = next(iter(val_dataloader))\n",
        "    encoder_input = batch[\"encoder_input\"].to(device)\n",
        "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
        "    decoder_input = batch[\"decoder_input\"].to(device)\n",
        "    decoder_mask  = batch[\"decoder_mask\"].to(device)\n",
        "\n",
        "    encoder_input_tokens = [tokenizer_src.id_to_token(idx) for idx in encoder_input[0].cpu().numpy()]\n",
        "    decoder_input_tokens = [tokenizer_tgt.id_to_token(idx) for idx in decoder_input[0].cpu().numpy()]\n",
        "\n",
        "    # check that the batch size is 1\n",
        "    assert encoder_input.size(\n",
        "        0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "    model_out = greedy_decode(\n",
        "        model, encoder_input, encoder_mask, vocab_src, vocab_tgt, config['seq_len'], device)\n",
        "\n",
        "    return batch, encoder_input_tokens, decoder_input_tokens"
      ],
      "metadata": {
        "id": "Cjz-EvB0CRdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
        "    return pd.DataFrame(\n",
        "        [\n",
        "            (\n",
        "                r,\n",
        "                c,\n",
        "                float(m[r, c]),\n",
        "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
        "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
        "            )\n",
        "            for r in range(m.shape[0])\n",
        "            for c in range(m.shape[1])\n",
        "            if r < max_row and c < max_col\n",
        "        ],\n",
        "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
        "    )\n",
        "\n",
        "def get_attn_map(attn_type: str, layer: int, head: int):\n",
        "    if attn_type == \"encoder\":\n",
        "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
        "    elif attn_type == \"decoder\":\n",
        "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
        "    elif attn_type == \"encoder-decoder\":\n",
        "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
        "    return attn[0, head].data\n",
        "\n",
        "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
        "    df = mtx2df(\n",
        "        get_attn_map(attn_type, layer, head),\n",
        "        max_sentence_len,\n",
        "        max_sentence_len,\n",
        "        row_tokens,\n",
        "        col_tokens,\n",
        "    )\n",
        "    return (\n",
        "        alt.Chart(data=df)\n",
        "        .mark_rect()\n",
        "        .encode(\n",
        "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
        "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
        "            color=\"value\",\n",
        "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
        "        )\n",
        "        #.title(f\"Layer {layer} Head {head}\")\n",
        "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
        "        .interactive()\n",
        "    )\n",
        "\n",
        "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int):\n",
        "    charts = []\n",
        "    for layer in layers:\n",
        "        rowCharts = []\n",
        "        for head in heads:\n",
        "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
        "        charts.append(alt.hconcat(*rowCharts))\n",
        "    return alt.vconcat(*charts)"
      ],
      "metadata": {
        "id": "W5SV1M9WCXrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
        "print(f'Source: {batch[\"src_text\"][0]}')\n",
        "print(f'Target: {batch[\"tgt_text\"][0]}')\n",
        "sentence_len = encoder_input_tokens.index(\"[PAD]\")"
      ],
      "metadata": {
        "id": "HCIOV7yfCe_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [0, 1, 2]\n",
        "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "# Encoder Self-Attention\n",
        "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))"
      ],
      "metadata": {
        "id": "4h-lRhYxCgcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Self-Attention\n",
        "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
      ],
      "metadata": {
        "id": "5KRjXrFICh37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder-Decoder Self-Attention\n",
        "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len)) # Cross-Attention Block"
      ],
      "metadata": {
        "id": "NgDCn1pwClsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_raw = load_dataset('cfilt/iitb-english-hindi') # please check the split parameter here\n",
        "\n",
        "train_ds_raw = ds_raw['train'].select(range(50000)) # in this way, we can select the no. of examples as much as desired\n",
        "val_ds_raw   = ds_raw['validation']\n",
        "test_ds_raw  = ds_raw['test']\n",
        "\n",
        "max_len_src=0\n",
        "max_len_tgt=0\n",
        "max_len_src2=0\n",
        "max_len_tgt2=0\n",
        "\n",
        "for item in val_ds_raw:\n",
        "        src_ids = tokenizer_src.encode(clean_text(item['translation'][config['lang_src']], config['lang_src'])).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(clean_text(item['translation'][config['lang_tgt']], config['lang_tgt'])).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "print(f'Max length of source sentence: {max_len_src}')\n",
        "print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "for item in val_ds_raw:\n",
        "        src_ids = tokenizer_src.encode(clean_text(item['translation'][config['lang_src']], config['lang_src'])).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(clean_text(item['translation'][config['lang_tgt']], config['lang_tgt'])).ids\n",
        "        max_len_src = max(max_len_src2, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt2, len(tgt_ids))\n",
        "\n",
        "print(f'Max length of source sentence: {max_len_src2}')\n",
        "print(f'Max length of target sentence: {max_len_tgt2}')"
      ],
      "metadata": {
        "id": "uH0otppx_vs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}